{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IP\\GuavaFinal\\guv_ver_constfix\\guv_ver\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import reduce\n",
    "from math import log\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = \"This is an example sentence\"\n",
    "# embeddings = model.encode(sentences)\n",
    "# print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dataset = pd.read_excel('Dataset.xlsx', sheet_name='Positive sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_final = pd.DataFrame(columns= ['Commit', 'CodePrev' 'PrevCodeVec', 'CodeNew', 'NewCodeVec', 'OldJdocVec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, row in df_dataset.iterrows():\n",
    "    \n",
    "#     ver = row['Version']\n",
    "#     commit_id = row['Commit']\n",
    "    \n",
    "#     sink = row['Sink(Doc)']\n",
    "#     sink_ind = sink.find('[')\n",
    "#     sink_path = sink[:sink_ind-1]\n",
    "#     sink_func = sink[sink_ind+1:-1]\n",
    "    \n",
    "#     src = row['Src(Func)']\n",
    "#     src_ind = src.find('[')\n",
    "#     src_path = src[:src_ind-1]\n",
    "#     src_func = src[src_ind+1:-1]\n",
    "    \n",
    "#     files = glob.glob('./**/**/')\n",
    "    \n",
    "#     # print(files) # '.\\\\v18.0\\\\032fcbc8f8c83f9779df0b697ae238468ba0335d\\\\'\n",
    "\n",
    "#     for file in tqdm(files):\n",
    "#         flag = False\n",
    "#         if commit_id in file:\n",
    "#             ind = file.find('\\\\', file.find('\\\\')+1)\n",
    "#             folder_name = file[:ind]\n",
    "#             # print(folder_name)\n",
    "#             csvs = glob.glob(folder_name+'.csv')\n",
    "#             for csv in csvs:\n",
    "#                 df_version = pd.read_csv(csv)\n",
    "#                 df_version = df_version.loc[(df_version['commit'] == commit_id) & (df_version['doc_pathinproj'] == sink_path) & (df_version['doc_methodsig'] == sink_func) & (df_version['func_pathinproj'] == src_path) & (df_version['func_methodsig'] == src_func)]\n",
    "#                 print(df_version['commit'], '  COMMIT ', commit_id)\n",
    "#                 print(df_version['doc_pathinproj'], '  DOC_PATH  ', sink_path)\n",
    "#                 print(df_version['doc_methodsig'], '  DOC_SIG ', sink_func)\n",
    "#                 print(df_version['func_pathinproj'], '  FUNC_PATH ', src_path)\n",
    "#                 print(df_version['func_methodsig'], '  FUNC_SIG  ', src_func)\n",
    "#                 # print(commit_id, \"   \", file, \"   \", csv, \"hello\")\n",
    "#                 # print(commit_id, folder_name)\n",
    "                \n",
    "#                 # print(df_version)\n",
    "                \n",
    "#                 # for _, rows in df_version.iterrows():\n",
    "#                 #     oldJdoc_path = rows['doc_oldJdoc']\n",
    "#                 #     indd = oldJdoc_path.find('/')\n",
    "#                 #     oldJdoc_path = os.path.join(file, oldJdoc_path[indd+1:])\n",
    "#                 #     print(oldJdoc_path)\n",
    "#                 #     with open(oldJdoc_path, 'r') as doc_file:\n",
    "#                 #         doc_content = doc_file.read()\n",
    "#                 #         embedding = model.encode(doc_content)\n",
    "#                 #         a_tensor = torch.Tensor(embedding)\n",
    "#                 #         temp_df = {'Commit': commit_id, 'OldJdoc':a_tensor}\n",
    "#                 #         df_final = df_final.append(temp_df, ignore_index = True)\n",
    "                        \n",
    "                        \n",
    "            \n",
    "                \n",
    "            \n",
    "#         #     flag = True\n",
    "#         # if flag:\n",
    "#         #     break\n",
    "        \n",
    "        \n",
    "#     # commit_id = row['Commit']\n",
    "#     # src = row['Src(Func)']\n",
    "#     # src_ind = src.find('[')\n",
    "#     # src_path = src[:src_ind-1]\n",
    "#     # src_func = src[src_ind+1:-1]\n",
    "#     # files = glob.glob('./**/**/')\n",
    "\n",
    "#     # for file in tqdm(files):\n",
    "#     #     if commit_id in file:\n",
    "#     #         ind = file.find('\\\\', file.find('\\\\')+1)\n",
    "#     #         folder_name = file[:ind]\n",
    "#     #         csvs = glob.glob(folder_name+'*.csv')\n",
    "#     #         for csv in csvs:\n",
    "#     #             df_version = pd.read_csv(csv)\n",
    "#     #             df_version = df_version.loc[(df_version['commit'] == commit_id) & (df_version['func_pathinproj'] == src_path) & (df_version['func_methodsig'] == src_func)]\n",
    "#     #             # print(commit_id, \"   \", file, \"   \", csv, \"hello\")\n",
    "#     #             # print(commit_id, folder_name, df_version.head())\n",
    "                \n",
    "#     #             for _, rows in df_version.iterrows():\n",
    "#     #                 oldfunc_path = rows['func_oldfunccode']\n",
    "#     #                 if (str(oldfunc_path)!='nan'):\n",
    "#     #                     oldfunc_path = str(oldfunc_path)\n",
    "#     #                     print(oldfunc_path)\n",
    "                        \n",
    "#     #                     indd = oldfunc_path.find('/')\n",
    "#     #                     oldfunc_path = os.path.join(file, oldfunc_path[indd+1:])\n",
    "#     #                     oldfunc_path = oldfunc_path[:-3]+'json'\n",
    "                        \n",
    "#     #                     f = open(oldfunc_path, 'r')\n",
    "#     #                     code_content = json.load(f)['data']\n",
    "#     #                     temp_df = {'Commit': commit_id, 'PrevCode':code_content}\n",
    "#     #                     df_final = df_final.append(temp_df, ignore_index = True)\n",
    "                    \n",
    "                    \n",
    "#                     # newfunc_path = rows['func_newfunccode']\n",
    "#                     # if (newfunc_path != ''):\n",
    "#                     #     indd = newfunc_path.find('/')\n",
    "#                     #     newfunc_path = os.path.join(file, newfunc_path[indd+1:])\n",
    "#                     #     with open(oldfunc_path, 'r') as code_file:\n",
    "#                     #         code_content = code_file.read()\n",
    "#                     #         temp_df = {'Commit': commit_id, 'CodeNew':code_content}\n",
    "#                     #         df_final = df_final.append(temp_df, ignore_index = True)\n",
    "                    \n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Commit CodePrevPrevCode CodeNew NewCode  \\\n",
      "0  032fcbc8f8c83f9779df0b697ae238468ba0335d              NaN     NaN     NaN   \n",
      "1  032fcbc8f8c83f9779df0b697ae238468ba0335d              NaN     NaN     NaN   \n",
      "2  032fcbc8f8c83f9779df0b697ae238468ba0335d              NaN     NaN     NaN   \n",
      "3  032fcbc8f8c83f9779df0b697ae238468ba0335d              NaN     NaN     NaN   \n",
      "4  032fcbc8f8c83f9779df0b697ae238468ba0335d              NaN     NaN     NaN   \n",
      "\n",
      "  OldJdoc CodeChange                                           PrevCode  \n",
      "0     NaN        NaN  0.61098063 0.3562704 0.5782616 -0.7126763 -0.3...  \n",
      "1     NaN        NaN  0.61098063 0.3562704 0.5782616 -0.7126763 -0.3...  \n",
      "2     NaN        NaN  0.61098063 0.3562704 0.5782616 -0.7126763 -0.3...  \n",
      "3     NaN        NaN  0.61098063 0.3562704 0.5782616 -0.7126763 -0.3...  \n",
      "4     NaN        NaN  0.61098063 0.3562704 0.5782616 -0.7126763 -0.3...  \n"
     ]
    }
   ],
   "source": [
    "# print(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(columns= ['Commit', 'PrevCode', 'NewCode', 'PrevJdoc', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "44\n",
      "68\n",
      "85\n",
      "146\n",
      "188\n",
      "286\n",
      "316\n",
      "334\n",
      "353\n"
     ]
    }
   ],
   "source": [
    "csvs = ['v18.0', 'v18.0_neg', 'v19.0', 'v19.0_neg', 'v21.0', 'v21.0_neg', 'v28.1', 'v28.1_neg', 'v30.1.1', 'v30.1.1_neg']\n",
    "\n",
    "# df = pd.DataFrame(columns= ['Commit', 'PrevCode', 'NewCode', 'OldJdoc'])\n",
    "\n",
    "commit_count_dic = {}\n",
    "\n",
    "for csv in csvs:\n",
    "    \n",
    "    df = pd.read_csv(csv+'.csv')\n",
    "    \n",
    "    df = df[['func_oldfunccode', 'func_newfunccode', 'doc_oldJdoc', 'commit']]\n",
    "    \n",
    "    for ind, row in df.iterrows():\n",
    "        \n",
    "        commit_id = row['commit']\n",
    "        \n",
    "        if commit_id in commit_count_dic.keys():\n",
    "            commit_count_dic[commit_id] += 1\n",
    "            if commit_count_dic[commit_id] > 5:\n",
    "                continue\n",
    "        else:\n",
    "            commit_count_dic[commit_id] = 1\n",
    "        \n",
    "        # if (commit_id == '69a75be1f1f8b15a03fe8aaf985aceadc7a10f41' or commit_id  == '9cd3ea9cedd399648488142f7e1eefd0f5a63866'):\n",
    "        #     continue\n",
    "        \n",
    "        oldfunc_path = row['func_oldfunccode']\n",
    "        newfunc_path = row['func_newfunccode']\n",
    "        oldJdoc_path = row['doc_oldJdoc']\n",
    "        \n",
    "        \n",
    "        if (pd.isna(oldJdoc_path)):\n",
    "            continue\n",
    "            \n",
    "            \n",
    "        if ('neg' in csv):\n",
    "            csv = csv[:-4]\n",
    "        \n",
    "        oldJdoc_path = os.path.join(csv, oldJdoc_path)\n",
    "        if (not os.path.exists(oldJdoc_path)):\n",
    "            continue\n",
    "        \n",
    "        with open(oldJdoc_path, 'r') as doc_file:\n",
    "            doc_content = doc_file.read()\n",
    "            embedding = model.encode(doc_content)\n",
    "            a_tensor = torch.Tensor(embedding)\n",
    "            a_vec = a_tensor.tolist()\n",
    "        \n",
    "        \n",
    "        if (pd.isna(oldfunc_path)):\n",
    "            prev_code_vec = [0]*384  \n",
    "        else:\n",
    "            oldfunc_path = os.path.join(csv, oldfunc_path)\n",
    "            oldfunc_path = oldfunc_path[:-3]+'json'\n",
    "            \n",
    "            if not (os.path.exists(oldfunc_path)):\n",
    "                continue\n",
    "            \n",
    "            f1 = open(oldfunc_path, 'r')\n",
    "            prev_code_content = json.load(f1)['data']\n",
    "            prev_code_vec = list(prev_code_content.split(\" \"))\n",
    "            prev_code_vec = [float(i) for i in prev_code_vec]\n",
    "            \n",
    "            \n",
    "        if (pd.isna(newfunc_path)):\n",
    "            new_code_vec = [0]*384\n",
    "        else:\n",
    "            newfunc_path = os.path.join(csv, newfunc_path)\n",
    "            newfunc_path = newfunc_path[:-3]+'json'\n",
    "            \n",
    "            if (not os.path.exists(newfunc_path)):\n",
    "                continue\n",
    "            \n",
    "            f2 = open(newfunc_path, 'r')\n",
    "            new_code_content = json.load(f2)['data']\n",
    "            new_code_vec = list(new_code_content.split(\" \"))\n",
    "            new_code_vec = [float(i) for i in new_code_vec]\n",
    "                \n",
    "        \n",
    "        \n",
    "        label = 1\n",
    "        if ('Codes_neg' in oldJdoc_path):\n",
    "            label = 0\n",
    "            \n",
    "        # print(len(prev_code_vec))\n",
    "        # print(a_tensor.shape)\n",
    "        \n",
    "        # break\n",
    "        \n",
    "        temp_dict = {'Commit':commit_id, 'PrevCode': [prev_code_vec], 'NewCode': [new_code_vec], 'PrevJdoc': [a_vec], 'Label':label}        \n",
    "        \n",
    "        # print(temp_dict)\n",
    "        \n",
    "        temp_df = pd.DataFrame(temp_dict)\n",
    "        # print(temp_df)\n",
    "        # break\n",
    "        final_df = pd.concat([final_df, temp_df], ignore_index=True)\n",
    "         \n",
    "        # temp_df = {'PrevCode':old_code, 'NewCode':new_code, 'PrevJdoc':old_Jdoc}       \n",
    "        # df_neg = df_neg.append(temp_df, ignore_index = True)\n",
    "    \n",
    "        \n",
    "    print(len(final_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353\n"
     ]
    }
   ],
   "source": [
    "print(len(final_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_df[final_df.Label==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_df[final_df.Label==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = final_df['Label'].value_counts().min()\n",
    "final_df = final_df.groupby('Label').head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('final_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = None\n",
    "y = None\n",
    "\n",
    "count = 0\n",
    "\n",
    "for ind,row in final_df.iterrows():\n",
    "    prev_code_vec = np.array(row['PrevCode'])\n",
    "    new_code_vec = np.array(row['NewCode'])\n",
    "    prev_doc_vec = np.array(row['PrevJdoc'])\n",
    "    \n",
    "    count+=1\n",
    "    \n",
    "    # print(len(prev_code_vec))\n",
    "    # print(len(new_code_vec))\n",
    "    # print(len(prev_doc_vec))\n",
    "    # print(count)\n",
    "    \n",
    "    if (count == 1):\n",
    "        X = np.array(np.hstack((prev_code_vec, new_code_vec, prev_doc_vec)))\n",
    "        y = np.array(row['Label'])\n",
    "        continue\n",
    "    \n",
    "    arr = np.hstack((prev_code_vec, new_code_vec, prev_doc_vec))\n",
    "    X = np.vstack((X, arr))\n",
    "    \n",
    "    y = np.vstack((y, row['Label']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1152,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.ravel(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1152)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IP\\GuavaFinal\\guv_ver_constfix\\guv_ver\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clf1 = MLPClassifier(hidden_layer_sizes=(384, 192, 48, 16, 2, 1), activation=\"relu\", max_iter=2000).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(X, y, model, kind):\n",
    "    print(f\"{kind} Accuracy is {model.score(X, y)*100} %\")\n",
    "    y_pred = model.predict(X)\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    print(f\"The confusion matrix is\")\n",
    "    print(cm)\n",
    "    class_wise_accuracies = (cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]).diagonal()\n",
    "    print(f\"Class-wise accuracies are {class_wise_accuracies}\")\n",
    "    \n",
    "    # plt.plot(model.loss_curve_)\n",
    "    # plt.plot(model.validation_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Train Accuracy is 98.4375 %\n",
      "The confusion matrix is\n",
      "[[94  2]\n",
      " [ 1 95]]\n",
      "Class-wise accuracies are [0.97916667 0.98958333]\n",
      "\n",
      "Testing\n",
      "Test Accuracy is 68.75 %\n",
      "The confusion matrix is\n",
      "[[19 13]\n",
      " [ 7 25]]\n",
      "Class-wise accuracies are [0.59375 0.78125]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training\")\n",
    "score(X_train, y_train, clf1, kind = \"Train\")\n",
    "print(\"\\nTesting\")\n",
    "score(X_test, y_test, clf1, kind = \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = MLPClassifier(hidden_layer_sizes=(384, 192, 48, 16, 2), activation=\"relu\", max_iter=2000).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Train Accuracy is 100.0 %\n",
      "The confusion matrix is\n",
      "[[96  0]\n",
      " [ 0 96]]\n",
      "Class-wise accuracies are [1. 1.]\n",
      "\n",
      "Testing\n",
      "Test Accuracy is 62.5 %\n",
      "The confusion matrix is\n",
      "[[17 15]\n",
      " [ 9 23]]\n",
      "Class-wise accuracies are [0.53125 0.71875]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training\")\n",
    "score(X_train, y_train, clf2, kind = \"Train\")\n",
    "print(\"\\nTesting\")\n",
    "score(X_test, y_test, clf2, kind = \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3 = LogisticRegression(random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Train Accuracy is 100.0 %\n",
      "The confusion matrix is\n",
      "[[96  0]\n",
      " [ 0 96]]\n",
      "Class-wise accuracies are [1. 1.]\n",
      "\n",
      "Testing\n",
      "Test Accuracy is 60.9375 %\n",
      "The confusion matrix is\n",
      "[[18 14]\n",
      " [11 21]]\n",
      "Class-wise accuracies are [0.5625  0.65625]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training\")\n",
    "score(X_train, y_train, clf3, kind = \"Train\")\n",
    "print(\"\\nTesting\")\n",
    "score(X_test, y_test, clf3, kind = \"Test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,input,H,output):\n",
    "        super(Net,self).__init__()\n",
    "        self.linear1=nn.Linear(input,H[0])\n",
    "        self.linear2=nn.Linear(H[0],H[1])\n",
    "        self.linear3=nn.Linear(H[1],H[2])\n",
    "        self.linear4=nn.Linear(H[2],H[3])\n",
    "        self.linear5=nn.Linear(H[3],H[4])\n",
    "        self.linear6=nn.Linear(H[4],output)\n",
    " \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.linear1(x)\n",
    "        x = nn.ReLU(x)  \n",
    "        x = self.linear2(x)\n",
    "        x = nn.ReLU(x)  \n",
    "        x = self.linear3(x)\n",
    "        x = nn.ReLU(x)  \n",
    "        x = self.linear4(x)\n",
    "        x = nn.ReLU(x)\n",
    "        x = self.linear5(x)\n",
    "        x = nn.ReLU(x)\n",
    "        x = self.linear6(x)\n",
    "        x = nn.Sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=Net(input_dim,[384, 192, 48, 16, 2],output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "loss_list = []\n",
    "for t in range(2000):\n",
    "    y_pred = clf(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss_list.append(loss.item())\n",
    "    clf.zero_grad()\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        for param in clf.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae999e0644fb3c0a445ba6a2fc0ddd0e0f4ee2cdb52b79fc5e77d02e16a14466"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
